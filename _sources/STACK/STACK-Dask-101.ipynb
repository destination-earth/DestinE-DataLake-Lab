{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/destination-earth/DestinE-DataLake-Lab/blob/main/img/DestinE-banner.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Licence**: MIT <br>\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/dask_horizontal.svg\" align=\"right\" width=\"25%\" alt=\"Dask logo\">\n",
    "\n",
    "# STACK service - Dask 101\n",
    "\n",
    "<div class=\"alert-info\">\n",
    "    <h4>Overview</h4>\n",
    "    <h5>Content</h5>\n",
    "    <li>Dask API introduction</li>\n",
    "    <li>dask.distributed</li>\n",
    "    <li>DestinE DataLake Dask Cluster</li>\n",
    "    <h5>Duration: 20 min.</h5>\n",
    "</div>\n",
    "</br>\n",
    "<div class=\"alert-warning\">\n",
    "Please make sure Python DEDL kernel is used, and run the mamba install command.\n",
    "</div>\n",
    "\n",
    "\n",
    "## What is [Dask](\"https://www.dask.org/\")?\n",
    "Dask is a Python library for parallel and distributed computing.\n",
    "\n",
    "Dask addresses the challenge of scaling Python code from a single machine to large clusters of machines. In the world of data science and scientific computing, Python is a popular language due to its ease of use and extensive libraries like NumPy, Pandas, and scikit-learn. However, these libraries often struggle to handle large datasets that exceed the memory capacity of a single machine or require parallel processing for efficient computation.\n",
    "\n",
    "Dask provides parallel computing capabilities and allows Python developers to work with larger-than-memory datasets by parallelizing computation across multiple cores within a single machine or distributing it across a cluster of machines. It achieves this by providing parallelized versions of familiar data structures like arrays, dataframes, and lists, which seamlessly integrate with existing Python code.\n",
    "\n",
    "Dask can run on your laptop or can be scaled out to full capacity to a cloud cluster or HPC system.\n",
    "\n",
    "It offeres a rich ecosystem of Python libraries to support use cases in multiple domains such as:\n",
    "* Geospatial\n",
    "* Finance\n",
    "* Astrophysics\n",
    "* Microbiology\n",
    "* Environmental science\n",
    "\n",
    "Check out the Dask [use cases](https://stories.dask.org/en/latest/) page that provides a number of sample workflows to see how others are using Dask to solve their problems.\n",
    "\n",
    "<img src=\"https://global.discourse-cdn.com/standard14/uploads/pangeo/original/1X/657e3c5e0885ee4e5c2062c58f9aa094fa4b14a4.png\" align=\"left\" width=\"15%\" style=\"margin-right: 10px;\" alt=\"Pangeologo\">\n",
    "\n",
    "Escpecially the [Pangeo](https://pangeo.io/index.html) community, a community of people working to enable big data geoscience, heavily relies on Dask as the core compute frameworks to be used. Why? Pangeo community early realised the need for scaleable software and tools to handle petabyte-scale datasets on HPC and cloud platforms.\n",
    "\n",
    "Dask provides **multi-core** and **distributed+parallel** execution on **larger-than-memory** datasets\n",
    "\n",
    "### References\n",
    "\n",
    "This tutorial builds upon the following reference documents supported and maintained by the Dask and Pangeo communities.\n",
    "* [Dask Tutorial](https://tutorial.dask.org/)\n",
    "* [Dask Examples](https://examples.dask.org/index.html)\n",
    "* [Pangeo Tutorials](https://github.com/pangeo-data/pangeo-tutorial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Core Library (APIs)\n",
    "Dask provides several APIs, also called collections, to enable distributed+parallel execution on larger-than-memory datasets.\n",
    "We can think of Dask's APIs at a high and a low level:\n",
    "\n",
    "<center>\n",
    "<img src=\"https://tutorial.dask.org/_images/high_vs_low_level_coll_analogy.png\" width=\"65%\" alt=\"High vs Low level clothes analogy\">\n",
    "</center>\n",
    "\n",
    "*  **High-level collections:**  Dask provides high-level **Array, Bag, and DataFrame**\n",
    "   collections that mimic NumPy, lists, and pandas but can operate in parallel on\n",
    "   datasets that don't fit into memory.\n",
    "* **Low-level collections:**  Dask also provides low-level **Tasks (Delayed and Futures)**\n",
    "   collections that give you finer control to build custom parallel and distributed computations.\n",
    "\n",
    "**In this tutorial we will focus on Dask Arrays and Tasks (Delayed and Futures). Please visit the [Dask Examples](https://examples.dask.org/index.html) and [Dask Tutorial](https://tutorial.dask.org/) for additional information.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dask.array - parallelized numpy\n",
    "\n",
    "Parallel, larger-than-memory, n-dimensional array using blocked algorithms. \n",
    "\n",
    "*  **Parallel**: Uses all of the cores on your computer\n",
    "*  **Larger-than-memory**:  Lets you work on datasets that are larger than your available memory by breaking up your array into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n",
    "*  **Blocked Algorithms**:  Perform large computations by performing many smaller computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.dask.org/en/stable/_images/dask-array.svg\" width=\"40%\" align=\"right\" style=\"margin-left: 10px;\">\n",
    "\n",
    "\n",
    "In other words, Dask Array implements a subset of the NumPy ndarray interface using blocked algorithms, cutting up the large array into many small arrays. This lets us compute on arrays larger than memory using all of our cores. We coordinate these blocked algorithms using Dask graphs.\n",
    "\n",
    "In this notebook, we'll build some understanding by implementing some blocked algorithms from scratch.\n",
    "We'll then use Dask Array to analyze large datasets, in parallel, using a familiar NumPy-like API.\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "* [Array documentation](https://docs.dask.org/en/latest/array.html)\n",
    "* [Array screencast](https://youtu.be/9h_61hXCDuI)\n",
    "* [Array API](https://docs.dask.org/en/latest/array-api.html)\n",
    "* [Array examples](https://examples.dask.org/array.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrays - Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dask array looks and feels a lot like a numpy array. However, a dask array doesn’t directly hold any data. Instead, it symbolically represents the computations needed to generate the data. Nothing is actually computed until the actual numerical values are needed. This mode of operation is called “lazy”; it allows one to build up complex, large calculations symbolically before turning them over the scheduler for execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to create a numpy array of all ones, we do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "shape = (1000, 4000)\n",
    "ones_np = np.ones(shape)\n",
    "ones_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array contains exactly 32 MB of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%.1f MB' % (ones_np.nbytes / 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s create the same array using dask’s array interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "ones = da.ones(shape)\n",
    "ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, but we didn't tell Dask how to split up the array, so it is not optimized for distributed computation.\n",
    "\n",
    "A crucial difference with Dask is that we must specify the chunks argument. \"Chunks\" describes how the array is split up over many sub-arrays.\n",
    "\n",
    "There are several ways to [specify chunks](https://docs.dask.org/en/stable/array-chunks.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_shape = (1000, 1000)\n",
    "ones = da.ones(shape, chunks=chunk_shape)\n",
    "ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we just see a symbolic representation of the array, including its shape, dtype, and chunksize. No data has been generated yet. When we call **.compute()** on a dask array, the computation is trigger and the dask array becomes a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand what happened when we called .compute(), we can visualize the **Dask graph**, the symbolic operations, that make up the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones.visualize(format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array has four chunks. To generate it, Dask calls np.ones four times and then concatenates this together into one array.\n",
    "\n",
    "Rather than immediately loading a Dask array (which puts all the data into RAM), it is more common to reduce the data somehow. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_ones = ones.sum()\n",
    "sum_of_ones.visualize(format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see Dask’s strategy for finding the sum. This simple example illustrates the beauty of Dask: it automatically designs an algorithm appropriate for custom operations with big data.\n",
    "\n",
    "If we make our operation more complex, the graph gets more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_calculation = (ones * ones[::-1, ::-1]).mean()\n",
    "fancy_calculation.visualize(format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dask.delayed - parallelize generic Python code\n",
    "What if you don't have an Dask array or Dask dataframe? Instead of having blocks where the function is applied to each block, you can decorate functions with `@delayed` and _have the functions themselves be lazy_. Rather than compute its result immediately, it records what needs to be computed as a task into a graph that we’ll run later on parallel hardware.\n",
    "\n",
    "This is a simple way to use Dask to parallelize existing codebases or build [complex systems](https://blog.dask.org/2018/02/09/credit-models-with-dask). \n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "* [Delayed documentation](https://docs.dask.org/en/latest/delayed.html)\n",
    "* [Delayed screencast](https://www.youtube.com/watch?v=SHqFmynRxVU)\n",
    "* [Delayed API](https://docs.dask.org/en/latest/delayed-api.html)\n",
    "* [Delayed examples](https://examples.dask.org/delayed.html)\n",
    "* [Delayed best practices](https://docs.dask.org/en/latest/delayed-best-practices.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical workfow Read-Transform-Write workflow are most often implemented as outlined hereafter.\n",
    "In general, most workflows containing a for-loop can benefit from dask.delayed.\n",
    "```python\n",
    "import dask\n",
    "    \n",
    "@dask.delayed\n",
    "def process_file(filename):\n",
    "    data = read_a_file(filename)\n",
    "    data = do_a_transformation(data)\n",
    "    destination = f\"results/{filename}\"\n",
    "    write_out_data(data, destination)\n",
    "    return destination\n",
    "\n",
    "results = []\n",
    "for filename in filenames:\n",
    "    results.append(process_file(filename))\n",
    "    \n",
    "dask.compute(results)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dask.delayed - Example\n",
    "For demonstration purposes we will create simple functions to perform simple operations like add two numbers together, but they sleep for a random amount of time to simulate real work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def inc(x):\n",
    "    time.sleep(0.1)\n",
    "    return x + 1\n",
    "\n",
    "def dec(x):\n",
    "    time.sleep(0.1)\n",
    "    return x - 1\n",
    "\n",
    "def add(x, y):\n",
    "    time.sleep(0.2)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run them like normal Python functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = inc(1)\n",
    "y = dec(2)\n",
    "z = add(x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These ran one after the other, in sequence. Note though that the first two lines inc(1) and dec(2) don’t depend on each other, we could have called them in parallel.\n",
    "\n",
    "We can call dask.delayed on these funtions to make them lazy. Rather than compute their results immediately, they record what we want to compute as a task into a graph that we’ll run later on parallel hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "inc = dask.delayed(inc)\n",
    "dec = dask.delayed(dec)\n",
    "add = dask.delayed(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling these lazy functions is now almost free. We’re just constructing a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = inc(1)\n",
    "y = dec(2)\n",
    "z = add(x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize(format='svg', rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run in parallel. Call .compute() when you want your result as a normal Python object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallelize Normal Python code**\n",
    "\n",
    "Now we use dask.delayed in a normal for-loop Python code as given in the example above. This generates graphs instead of doing computations directly, but still looks like the code we had before. Dask is a convenient way to add parallelism to existing workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "zs = []\n",
    "for i in range(256):\n",
    "    x = inc(i)\n",
    "    y = dec(x)\n",
    "    z = add(x, y)\n",
    "    zs.append(z)\n",
    "\n",
    "zs = dask.persist(*zs)   # trigger computation in the background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Cluster (dask.distributed)\n",
    "Dask has the ability to run work on multiple machines using the distributed scheduler. `dask.distributed` is a lightweight library for distributed computing in Python. It extends both the `concurrent.futures` and Dask APIs to run on various clusters technologies such as Kubernetes, Yarn, SLURM, PBS, etc. .\n",
    "Most of the times when you are using Dask, you will be using a distributed scheduler, which exists in the context of a Dask cluster. When we talk about Dask Clusters we can think of those as depicted in the following:\n",
    "\n",
    "<center>\n",
    "<img src=\"https://tutorial.dask.org/_images/distributed-overview.png\" width=\"75%\" alt=\"Distributed overview\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask @ DEDL\n",
    "DestinE Data Lake utilises a deployment of [Dask Gateway](https://gateway.dask.org/) on each location (bridge) in the data lake. Dask Gateway provides a secure, multi-tenant server for managing Dask clusters. It allows users to launch and use Dask clusters in a shared, centrally managed cluster environment, without requiring users to have direct access to the underlying cluster backend (e.g. Kubernetes, Hadoop/YARN, HPC Job queues, etc…).\n",
    "\n",
    "Dask Gateway exposes a REST API to spawn clusters on demand. The overall architecture of Dask Gateway is depicted hereafter. \n",
    "<img src=\"https://gateway.dask.org/_images/architecture.svg\">\n",
    "\n",
    "### How to connect and spawn a cluster?\n",
    "\n",
    "**Central Site**\n",
    "* address: http://dask.central.data.destination-earth.eu\n",
    "* proxy_address: tcp://dask.central.data.destination-earth.eu:80\n",
    "\n",
    "**LUMI Bridge**\n",
    "* address: http://dask.lumi.data.destination-earth.eu\n",
    "* proxy_address: tcp://dask.lumi.data.destination-earth.eu:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask_gateway.auth import GatewayAuth\n",
    "from getpass import getpass\n",
    "from destinelab import AuthHandler as DESP_AuthHandler\n",
    "\n",
    "class DESPAuth(GatewayAuth):\n",
    "    def __init__(self, username: str):\n",
    "        self.auth_handler = DESP_AuthHandler(username, getpass(\"Please input your DESP password: \"))\n",
    "        self.access_token = self.auth_handler.get_token()\n",
    "    \n",
    "    def pre_request(self, _):\n",
    "        headers = {\"Authorization\": \"Bearer \" + self.access_token}\n",
    "        return headers, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only authenticated access is granted to the DEDL STACK service Dask, therefore a helper class to authenticate a user against the DESP identity management system is implemented. The users password is directly handed over to the request object and is not permanently stored.\n",
    "\n",
    "In the following, please enter your DESP username and password. Again, the password will only be saved for the duration of this user session and will be remove as soon as the notebook/kernel is closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rich.prompt import Prompt\n",
    "myAuth = DESPAuth(username=Prompt.ask(prompt=\"Username\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "gateway = Gateway(address=\"http://dask.central.data.destination-earth.eu\",\n",
    "                  proxy_address=\"tcp://dask.central.data.destination-earth.eu:80\",\n",
    "                  auth=myAuth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster creation and client instantiation to communicate with the new cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = gateway.new_cluster()\n",
    "client = cluster.get_client()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now the cluster will only consist of the distributed scheduler. If you want to spawn workers directly via Python adaptively, please use the following method call. With the following the cluster will be scaled to 2 workers initially. Depending on the load, Dask will add addtional workers, up to 5, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.adapt(minimum=2, maximum=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### dask.futures - non-blocking distributed calculations\n",
    "We will now make use of the remote Dask Cluster using the Dask low-level collection dask.futures.\n",
    "\n",
    "Submit arbitrary functions for computation in a parallelized, eager, and non-blocking way.\n",
    "\n",
    "The `futures` interface (derived from the built-in `concurrent.futures`) provide fine-grained real-time execution for custom situations. We can submit individual functions for evaluation with one set of inputs, or evaluated over a sequence of inputs with `submit()` and `map()`. The call returns immediately, giving one or more *futures*, whose status begins as \"pending\" and later becomes \"finished\". There is no blocking of the local Python session.\n",
    "\n",
    "**Important**\n",
    "\n",
    "This is the important difference between futures and delayed. Both can be used to support arbitrary task scheduling, but delayed is lazy (it just constructs a graph) whereas futures are eager. With futures, as soon as the inputs are available and there is compute available, the computation starts. \n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "* [Futures documentation](https://docs.dask.org/en/latest/futures.html)\n",
    "* [Futures screencast](https://www.youtube.com/watch?v=07EiCpdhtDE)\n",
    "* [Futures examples](https://examples.dask.org/futures.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same workflow that as given above in the ``dask.delayed`` section. It is a for-loop to iterate of certain files to perform a transformation and to write the result.\n",
    "\n",
    "```python\n",
    "def process_file(filename):\n",
    "    data = read_a_file(filename)\n",
    "    data = do_a_transformation(data)\n",
    "    destination = f\"results/{filename}\"\n",
    "    write_out_data(data, destination)\n",
    "    return destination\n",
    "\n",
    "futures = []\n",
    "for filename in filenames:\n",
    "    future = client.submit(process_file, filename)\n",
    "    futures.append(future)\n",
    "    \n",
    "futures\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run these function locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inc(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can submit them to run remotely with Dask. This immediately returns a future that points to the ongoing computation, and eventually to the stored result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "future = client.submit(inc, 1)  # returns immediately with pending future\n",
    "future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wait a second, and then check on the future again, you’ll see that it has finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can block on the computation and gather the result with the .result() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other ways to wait for a future**\n",
    "```python\n",
    "from dask.distributed import wait, progress\n",
    "progress(future)\n",
    "```\n",
    "shows a progress bar in the notebook. This progress bar is also asynchronous, and doesn't block the execution of other code in the meanwhile.\n",
    "\n",
    "```python\n",
    "wait(future)\n",
    "```\n",
    "blocks and forces the notebook to wait until the computation pointed to by `future` is done. However, note that if the result of `inc()` is sitting in the cluster, it would take **no time** to execute the computation now, because Dask notices that we are asking for the result of a computation it already knows about. More on this later.\n",
    "\n",
    "**Other ways to gather results**\n",
    "```python\n",
    "client.gather(futures)\n",
    "```\n",
    "gathers results from more than one future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import wait, progress\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "future_x = client.submit(inc, 1)\n",
    "future_y = client.submit(inc, 2)\n",
    "future_z = client.submit(sum, [future_x, future_y])\n",
    "progress(future_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "Remove your cluster again to free up resources when you are done.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.close(shutdown=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python DEDL",
   "language": "python",
   "name": "python_dedl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
