{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0965390-b362-4295-877b-63842eda4fb4",
   "metadata": {},
   "source": [
    "![](https://github.com/destination-earth/DestinE-DataLake-Lab/blob/main/img/DestinE-banner.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f34b7e6-0fd0-43ff-ad52-d1e705e9cd06",
   "metadata": {},
   "source": [
    "**Licence**: MIT <br>\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/dask_horizontal.svg\" align=\"right\" width=\"25%\" alt=\"Dask logo\">\n",
    "\n",
    "# STACK service - Python Client Dask\n",
    "# Multi-cloud processing with Dask\n",
    "\n",
    "<div class=\"alert-info\">\n",
    "    <h4>Overview</h4>\n",
    "    <h5>Content</h5>\n",
    "    <li>DestinE Data Lake (DEDL) Stack Client</li>\n",
    "    <li>Making use of clients context manager</li>\n",
    "    <li>Use Case: Pakistan Flood 2022</li>\n",
    "    <h5>Duration: 15 min.</h5>\n",
    "</div>\n",
    "</br>\n",
    "<div class=\"alert-warning\">\n",
    "Please make sure Python DEDL kernel is used.\n",
    "</div>\n",
    "\n",
    "DestinE Data Lake utilises a deployment of [Dask Gateway](https://gateway.dask.org/) on each location (bridge) in the data lake. Dask Gateway provides a secure, multi-tenant server for managing Dask clusters. It allows users to launch and use Dask clusters in a shared, centrally managed cluster environment, without requiring users to have direct access to the underlying cluster backend (e.g. Kubernetes, Hadoop/YARN, HPC Job queues, etc…).\n",
    "\n",
    "Dask Gateway exposes a REST API to spawn clusters on demand. The overall architecture of Dask Gateway is depicted hereafter.\n",
    "\n",
    "<img src=\"https://github.com/destination-earth/DestinE-DataLake-Lab/blob/main/STACK/img/DEDL_Dask_multicloud.png?raw=true\"  width=\"60%\">\n",
    "\n",
    "## DEDL Dask Gateway\n",
    "\n",
    "**Central Site**\n",
    "* address: http://dask.central.data.destination-earth.eu\n",
    "* proxy_address: tcp://dask.central.data.destination-earth.eu:80\n",
    "\n",
    "**LUMI Bridge**\n",
    "* address: http://dask.lumi.data.destination-earth.eu\n",
    "* proxy_address: tcp://dask.lumi.data.destination-earth.eu:80\n",
    "\n",
    "Only authenticated access is granted to the DEDL STACK service Dask, therefore a helper class to authenticate a user against the DESP identity management system is implemented. The users password is directly handed over to the request object and is not permanently stored.\n",
    "\n",
    "In the following, please enter your DESP username and password. Again, the password will only be saved for the duration of this user session and will be remove as soon as the notebook/kernel is closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9700de1b-a0a4-4e01-bd27-1dbeab161ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway.auth import GatewayAuth\n",
    "from getpass import getpass\n",
    "from destinelab import AuthHandler as DESP_AuthHandler\n",
    "\n",
    "class DESPAuth(GatewayAuth):\n",
    "    def __init__(self, username: str):\n",
    "        self.auth_handler = DESP_AuthHandler(username, getpass(\"Please input your DESP password: \"))\n",
    "        self.access_token = self.auth_handler.get_token()\n",
    "    \n",
    "    def pre_request(self, _):\n",
    "        headers = {\"Authorization\": \"Bearer \" + self.access_token}\n",
    "        return headers, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a000350-f099-4f2d-b815-a3d730b6cf4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Username: </pre>\n"
      ],
      "text/plain": [
       "Username: "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " christoph.reimer\n",
      "Please input your DESP password:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response code: 200\n"
     ]
    }
   ],
   "source": [
    "from rich.prompt import Prompt\n",
    "myAuth = DESPAuth(username=Prompt.ask(prompt=\"Username\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc5076d-9a05-41ae-ba88-724f83fe6f19",
   "metadata": {},
   "source": [
    "## DestinE Data Lake (DEDL) Stack Client\n",
    "\n",
    "The [DEDL Stack Client is a Python](https://github.com/destination-earth/DestinE_EUMETSAT_DEDL_Stack_Client) library to facilitate the use of Stack Service Dask. The main objective is to provide an abstraction layer to interact with the various clusters on each DEDL bridge. Computations can be directed to the different Dask clusters by making use of a context manager as given in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c99a4b73-ad8c-43d3-af75-b98199610edc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new cluster for Central Site\n",
      "Create new cluster for LUMI Bridge\n"
     ]
    }
   ],
   "source": [
    "from dedl_stack_client.dask import DaskMultiCluster\n",
    "\n",
    "myDEDLClusters = DaskMultiCluster(auth=myAuth)\n",
    "myDEDLClusters.new_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3007ebbb-504f-4fc8-b026-e638124daea6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dask.central.data.destination-earth.eu/clusters/dask-gateway.203d0155443945c986a35b0488db7ebb/status\n",
      "http://dask.lumi.data.destination-earth.eu/clusters/dask-gateway.9010455c0c8a44df9a52e15003252889/status\n"
     ]
    }
   ],
   "source": [
    "myDEDLClusters.get_cluster_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b6d5b-733c-4f41-8fbd-c7ea0a8bd380",
   "metadata": {},
   "source": [
    "We can again showcase the execution of standard Python functions on the remote clusters.\n",
    "In the following we will make use of `dask.futures`, non-blocking distributed calculations, utilising the `map()` method for task distribution. Detailed information about `dask.futures` can be found on the [Dask documention](https://docs.dask.org/en/stable/futures.html).\n",
    "\n",
    "This approach allows for embarrassingly parallel task scheduling, which is very similar to Function as a Service capabilities.\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/map-reduce-task-scheduling.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "636b5c1a-f16b-41de-9393-ca0afe87a2c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import sleep as wait\n",
    "\n",
    "def apply_myfunc(x):\n",
    "    wait(1)\n",
    "    return x+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56504b15-1153-4e62-b6e3-46def0c1ee72",
   "metadata": {},
   "source": [
    "We want to run `apply_myfunc()` on **Central Site** and wait for all results to be ready. `my_filelist_central` represents a filelist to be processed by `apply_myfunc()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b627355-1b91-45b4-ae37-981060f17179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_filelist_central = range(20)\n",
    "with myDEDLClusters.as_current(location=\"central\") as myclient:\n",
    "    central_future = myclient.map(apply_myfunc, my_filelist_central)\n",
    "    results_central = myclient.gather(central_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2d0ec97-ed19-417a-85f6-d98d5b3a0cd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_central"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea02c98-a9f3-4d7f-92e1-68474247364f",
   "metadata": {},
   "source": [
    "Run computation at **LUMI bridge**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3a4b102-327f-4842-8c38-d551cf720a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_filelist_lumi = range(32)\n",
    "with myDEDLClusters.as_current(location=\"lumi\") as myclient:\n",
    "    lumi_future = myclient.map(apply_myfunc, my_filelist_lumi)\n",
    "    results_lumi = myclient.gather(lumi_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12e42db7-953b-45fa-a7d7-ca5c6dc31653",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_lumi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe9dbd-0e3b-47e4-8be6-2dadf7d6b4a3",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "* Python libraries use in the local environment need to match, same version, with those available in the Dask Cluster. If this is not the case, you will get a warning, code might work but not guaranteed.\n",
    "* No direct data exchange between Dask Workers across cloud locations possible. Each location acts as atmoic unit, however data can be easily exchanged via storage services such as S3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5877b850-ca04-4c0b-9a55-a40c1924fde5",
   "metadata": {},
   "source": [
    "## Use Case example: Pakistan Flood 2022\n",
    "\n",
    "**The complete use case is available on GitHub via https://github.com/destination-earth/DestinE_EUMETSAT_PakistanFlood_2022.**\n",
    "\n",
    "The use case demonstrates the multi-cloud capabilities of DEDL following the paradigm of data proximate computing. Data of the **Global Flood Monitoring (GFM)** service as well as **Climate DT outputs, simulated by utilising ERA5 data** have been use for flood risk assessment.\n",
    "\n",
    "<img src=\"https://github.com/destination-earth/DestinE_EUMETSAT_PakistanFlood_2022/blob/main/img/DEDL_FloodUC_dataflow.png?raw=true\" width=35%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c33228-618e-42aa-bacd-5edd3089a3d4",
   "metadata": {},
   "source": [
    "Data is stored as datacubes (zarr format) at Central Site and at LUMI bridge in object storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c20ebe7-84b0-4ed4-a1c6-a54889b4fe8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import xarray as xr\n",
    "\n",
    "xr.set_options(keep_attrs=True)\n",
    "\n",
    "s3fs_central = s3fs.S3FileSystem(\n",
    "    anon=True,\n",
    "    use_ssl=True,\n",
    "    client_kwargs={\"endpoint_url\": \"https://s3.central.data.destination-earth.eu\"})\n",
    "\n",
    "s3fs_lumi = s3fs.S3FileSystem(\n",
    "    anon=True,\n",
    "    use_ssl=True,\n",
    "    client_kwargs={\"endpoint_url\": \"https://s3.lumi.data.destination-earth.eu\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21638d2d-b3c7-4967-9a06-9ed4194991e5",
   "metadata": {},
   "source": [
    "We can list the data available at Central Site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32f57a78-fce8-4eed-8d91-0d8ad90e0896",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['increment1-testdata/2022-08-30.zarr', 'increment1-testdata/built_data.zarr']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3fs_central.ls(\"increment1-testdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e9c11-e177-4732-9af3-8bd353e1e97b",
   "metadata": {},
   "source": [
    "Read data stored in S3 bucket at Central Site. The data we want to read is a single Zarr data store representing the GFM flood data over Pakistan for 2022-08-30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "471ada19-2d5a-4aca-9503-53bc8efbb98e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flood_map = xr.open_zarr(store=s3fs.S3Map(root=f\"increment1-testdata/2022-08-30.zarr\", s3=s3fs_central, check=False),\n",
    "                         decode_coords=\"all\",)[\"flood\"].assign_attrs(location=\"central\", resolution=20)\n",
    "#flood_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bc0e01-f394-474f-badb-7947ad5539b8",
   "metadata": {},
   "source": [
    "We now want to run simple computation and compute the flooded area for the this day in August 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e31c193-4e06-4ab2-8a5f-55ecb7b6257a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flooded_area_ = flood_map.sum()*20*20/1000.\n",
    "#flooded_area_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3384b1-7716-4dac-9a59-81be299a76d4",
   "metadata": {},
   "source": [
    "So far we haven't computed anything, so lets do the computation now on the Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4dbf895e-d9ae-4ecf-b6ee-abac2c28bc3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Flooded area: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18991614.0</span> km2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Flooded area: \u001b[1;36m18991614.0\u001b[0m km2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "from rich.prompt import Prompt\n",
    "console = Console()\n",
    "\n",
    "flooded_area = myDEDLClusters.compute(flooded_area_, sync=True)\n",
    "console.print(f\"Flooded area: {flooded_area.data} km2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aee5f8-12bf-4d34-b645-ea371e70f1f3",
   "metadata": {},
   "source": [
    "**How was that processing routed to Dask Gateway at Central Site**?\n",
    "\n",
    "`myDEDLClusters.compute(flooded_area_, sync=True)` checks for annotations (attributes) of array and maps that to available Dask Clusters.\n",
    "\n",
    "\n",
    "Preprocess GFM data at **Central Site** for visualiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad9d15f4-c26a-4ea3-a51f-848c3dc77994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(data_array: xr.DataArray, method: str):\n",
    "    data_array = data_array.squeeze()\n",
    "    steps = 500 // data_array.attrs[\"resolution\"]\n",
    "    coarsened = data_array.coarsen({'y': steps, 'x': steps}, boundary='trim')\n",
    "    if method == 'median':\n",
    "        data_array = (coarsened.median() > 0).astype('float32')\n",
    "    elif method == 'mean':\n",
    "        data_array = coarsened.mean()\n",
    "    elif method == 'max':\n",
    "        data_array = coarsened.max()\n",
    "    else:\n",
    "        raise NotImplementedError(method)\n",
    "    return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a159d8f-b4b8-4b40-b0f4-8412ff054612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flood_prep_ = preprocess_dataset(flood_map, 'median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12d0a7a9-b747-422a-b816-68ef4e5f3bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "flood_prep = myDEDLClusters.compute(flood_prep_, sync=True)\n",
    "flood_prep.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "flood_prep = flood_prep.rio.reproject(f\"EPSG:3857\", nodata=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7884affc-fc29-41e0-9c3a-fdbed6fb09e6",
   "metadata": {},
   "source": [
    "Visualise flood data on map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01cc1d46-f3e9-4302-aafe-650d09463d73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9c7249181848be8d4dd6f40304ec7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[26.0, 68.0], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', 'zoom_out…"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import leafmap\n",
    "from attr import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Extent:\n",
    "    min_x: float\n",
    "    min_y: float\n",
    "    max_x: float\n",
    "    max_y: float\n",
    "    crs: str\n",
    "    def get_center(self):\n",
    "        return (np.mean([self.min_y, self.max_y]),\n",
    "                np.mean([self.min_x,self.max_x]))\n",
    "\n",
    "\n",
    "roi_extent = Extent(65, 21, 71, 31, crs='EPSG:4326')\n",
    "\n",
    "m = leafmap.Map(center=roi_extent.get_center(),\n",
    "                zoom=8, height=600)\n",
    "m.add_raster(flood_prep, colormap=\"Blues\", layer_name=\"Flood\", nodata=0.)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9880bc2-50d4-4274-b1d4-299d88e956a4",
   "metadata": {},
   "source": [
    "Read data stored in S3 bucket at LUMI bridge (Finland). Data we want to read is a datacube generated from ERA-5 representing predicted rainfall data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "afc9e489-7916-41ee-8086-a16fa0e22852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rainfall = xr.open_zarr(store=s3fs.S3Map(root=f\"increment1-testdata/predicted_rainfall.zarr\",\n",
    "                                         s3=s3fs_lumi,\n",
    "                                         check=False),\n",
    "                        decode_coords=\"all\",)[\"tp\"].assign_attrs(location=\"lumi\", resolution=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d8eba-d873-4b5f-b898-86e1e682ae5c",
   "metadata": {},
   "source": [
    "And again run the computation close to the data, therefore at **LUMI bridge**.\n",
    "\n",
    "First we compute the accumulated rainfall over Pakistan.\n",
    "Secondly we compute the average rainfall for August 2022 (monthly mean) at global scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1dd6382f-7610-42e7-98aa-009c67c168c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def accum_rain_predictions(rain_data, startdate, enddate, extent):\n",
    "    rain_ = rain_data.sel(time=slice(startdate, enddate),\n",
    "                          latitude=slice(extent.max_y, extent.min_y),\n",
    "                          longitude=slice(extent.min_x, extent.max_x))\n",
    "    return rain_.cumsum(dim=\"time\", keep_attrs=True)*1000\n",
    "\n",
    "# compute accumulated rainfall over Pakistan\n",
    "acc_rain_ = accum_rain_predictions(rainfall, startdate=datetime(2022, 8, 18),\n",
    "                                                  enddate=datetime(2022, 8, 30),\n",
    "                                                  extent=roi_extent)\n",
    "acc_rain_ = acc_rain_.rename({\"longitude\":\"x\", \"latitude\":\"y\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "39234129-7714-440d-8c8c-c2f82239473d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_rain = myDEDLClusters.compute(acc_rain_, sync=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "737de608-8495-4ecd-a046-33023a55d10b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def acc_rain_reproject(rain):\n",
    "    from rasterio.enums import Resampling\n",
    "    rain.rio.write_nodata(0, inplace=True)\n",
    "    rain.rio.write_crs('EPSG:4326', inplace=True)\n",
    "    return rain.rio.reproject('EPSG:3857', resolution=500, resampling=Resampling.bilinear)\n",
    "\n",
    "acc_rain = acc_rain_reproject(acc_rain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5279b6bc-4f89-44b9-bc8e-e99143183512",
   "metadata": {},
   "source": [
    "Visualise forecast data provided by the Digital Twin which could have been used for flood risk assessment or even alerting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d6342f89-8f88-4f19-9e12-a5b3d9171f89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_dim_len = acc_rain.shape[0]\n",
    "for day in range(0, time_dim_len):\n",
    "    fpath_str = f\"./{day}.tif\"\n",
    "    acc_rain[day,:].rio.to_raster(fpath_str,\n",
    "                                  driver=\"COG\",\n",
    "                                  overview_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fcca5a-59ea-4fc7-97f1-58def2b402ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import leafmap\n",
    "from localtileserver import get_leaflet_tile_layer\n",
    "\n",
    "m = leafmap.Map(center=roi_extent.get_center(),\n",
    "                zoom=6, height=600)\n",
    "\n",
    "layer_dict = {}\n",
    "date_vals = np.datetime_as_string(acc_rain[\"time\"].values, unit='D')\n",
    "for day in range(0, time_dim_len):\n",
    "    layer_dict[date_vals[day]]= get_leaflet_tile_layer(f\"./{day}.tif\",\n",
    "                                                       colormap=\"Blues\",\n",
    "                                                       indexes=[1],\n",
    "                                                       nodata=0.,\n",
    "                                                       vmin=acc_rain.min().values,\n",
    "                                                       vmax=acc_rain.max().values,\n",
    "                                                       opacity=0.85)\n",
    "\n",
    "m.add_local_tile(flood_prep,\n",
    "                 colormap=\"Blues\",\n",
    "                 nodata=0.)\n",
    "m.add_time_slider(layer_dict,\n",
    "                  layer=\"Accumluated Rainfall\",\n",
    "                  time_interval=1.)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e88d72e0-72e1-4b14-98e7-5ae81cb3769f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "myDEDLClusters.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095e2c3-f2fa-47da-a3c8-2375c94e1f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python DEDL",
   "language": "python",
   "name": "python_dedl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}